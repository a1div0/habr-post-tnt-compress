# Подробнее о сжатии данных в Tarantool 2.10
Всем привет! Хотелось бы подробнее рассказать про новую фичу, реализованную в Enterprise-версии
Тарантула, начиная с 2.10 RC-1 - сжатие данных (в оперативной памяти). Мы рассмотрим
что она может и чего не может, как применять и особенности данного функционала.

> Читать дальше

## Как включить
Доступно два варианта сжатия:
* zstd
* lz4

Функционал достаточно прост в использовании - нужно только указать в настройках
поля, что его надо сжимать: `compression = '<вариант>'`

Например:
```lua
local space = box.schema.space.create(space_name, { if_not_exists = true })
space:format({
    {name = 'uid', type = 'string'},
    {name = 'body', type = 'string', compression = 'zstd'},
})
```

Если это не новый проект, а существующий проект с данными, то нужно ещё вызвать фоновую
миграцию:
```lua
space:upgrade({ background = true })
```

Подробнее [здесь](https://www.tarantool.io/en/enterprise_doc/tuple_compression/)

## Реальные результаты
Давайте посмотрим, что получается на примере реальных данных, одной из телеком компаний. Реальные
данные представляют собой 100 тыс. документов объёмом 316 МБ. Они представляют из себя JSON-ы
разных размеров.

Теперь по очереди применим каждый метод. Пишем выборку из 100 тыс. документов. CPU = 3,6 ГГц.
Чтобы было проще оценить, добавим для сравнения работу внешней библиотеки ZLIB.

Исходник теста можно посмотреть [тут](https://github.com/a1div0/habr-post-tnt-compress/blob/master/one_instance.lua).

| Метод | Время упаковки и записи в таблицу, сек | Время чтения и распаковки, сек | Конечный размер спейса, МБ | Коэффициент сжатия | Сжатие, % |
| ----- | ------ | ---- | ---- | ---- | ---- |
| ZSTD | 61.222 | 0.889 | 113.234 | 2.790 | 64.16 |
| LZ4 | 7.701 | 0.963 | 186.499 | 1.694 | 40.97 |
| ZLIB* | 16.119 | 3.528 | 121.958 | 2.590 | 61.40 |

`*` Внешняя библиотека ZLIB

(тут добавить гистограмму "время сжатия и записи")
(тут добавить гистограмму "время чтения и распаковки")
(тут добавить гистограмму "размер несжатый и сжатый")
(тут добавить гистограмму "сжатие %")

## Сжатие в кластере
В реальных продуктовых проектах экземпляры Тарантула разделены по ролям. Для
нашего примера важно выделить две:
* маршрутизатор (router)
* хранилище (storage)

В рамках кластера организовать сжатие можно разными способами - по месту сжатия и разжатия. И в каждом
способе есть свои плюсы и минусы. Рассмотрим три варианта.

### Вариант первый - быстрый эффект
![](cluster-1.jpg)

Сжимаем и разжимаем в хранилище. Этот вариант обеспечивает описываемый в статье
функционал. Это подходит, если вам нужно быстро, при ничтожных изменениях проекта, высвободить кучу
места.

Звучит здорово, какая цена? Дополнительная нагрузка на ЦП хранилища, его замедление. Более того,
каждая реплика будет повторять то же самое сжатие. Как следствие - это снижает производительность
кластера. Хранилище со встроенным упаковщиком дороже масштабировать. То есть вместо дополнительного
только упаковщика, вам надо рядом поставить хранилище с упаковщиком.

Если вы хотите сохранить производительность кластера, пока что потребуется больше усилий.

### Вариант второй - максимальная производительность
![](cluster-2.jpg)

Cжимаем и разжимаем на роутере. Это даёт больше производительности, но здесь встроенное сжатие никак
не поможет. Необходимо подключать внешнюю библиотеку и вот это вот всё. И в мастер и на реплики
будут попадать уже упакованные данные.

Плюсы:
* не нагружаем хранилище
* уменьшаем трафик между роутером и стораджем
* легче масштабируем - ставим нужное количество роутеров с упаковщиком до требуемой производительности

Минусы:
* нужно реализовать логику упаковщика на роутере

Если хочется большего, например поставить меньше экземпляров, без особой потери производительности,
можно рассмотреть вариант 3.

### Вариант третий - ленивое сжатие
![](cluster-3.jpg)

Сжимаем на отдельном экземпляре, разжимаем на роутере. То есть пишем в хранилище как есть с признаком
"не упакован". Далее реализуем отдельную роль "упаковщик", которая проходит по мастер-хранилищам и
упаковывает не упакованное. Если данные поступают быстро и надо паковать быстрее - добавляем ещё
экземпляры упаковщика. На реплики будут попадать уже упакованные данные.

Плюсы:
* процесс сжатия масштабируется максимально выгодно
* меньше экземпляров кластера
* не нагружаем хранилище как в первом варианте

Минусы:
* дополнительный трафик в кластере - между хранилищем и упаковщиком
* некоторая дополнительная нагрузка на хранилище - плюс по одной операции чтения и записи
* нужно реализовать отдельную роль - упаковщик
* нужно реализовать логику распаковки на роутере

### Эффект переноса сжатия на роутер
На фоне предложенных вариантов хочется проследить эффект от переноса сжатия
с хранилища на роутер. Давайте соберём маленький кластер - пусть это будет два экземпляра - роутер и
хранилище. И попробуем его нагрузить при разных способах упаковки.

Реализация роутера - [тут]().
Реализация хранилища - [тут]().
Для нагрузочного обстрела на запись модуль - [тут](). И для него вспомогательный модуль
для генерации данных - [тут]().

